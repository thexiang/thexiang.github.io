{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words in `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:/Users/Jeff/udacity/Intro_to_Machine_Learning/ud120-projects/tools/\")\n",
    "sys.path.append('C:/Users/Jeff/udacity/Intro_to_Machine_Learning/ud120-projects/choose_your_own')\n",
    "sys.path.append('C:/Users/Jeff/udacity/Intro_to_Machine_Learning/ud120-projects/datasets_questions')\n",
    "\n",
    "import os\n",
    "os.chdir('C:/Users/Jeff/udacity/Intro_to_Machine_Learning/ud120-projects/text_learning')\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "string1 = '''Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec pharetra ornare lacinia. \n",
    "Cras et facilisis sem. Mauris fermentum nec dui at aliquam. Mauris ante justo, aliquam \n",
    "eget facilisis ac, elementum eget dolor. Nulla lacinia ac lorem vel tempus. Vivamus condimentum \n",
    "vulputate semper. Nullam dictum, mauris dignissim auctor suscipit, dui turpis elementum turpis, \n",
    "et tempor urna diam sed nisl. Vestibulum mollis quam at nisl egestas tincidunt. Nullam ut nibh \n",
    "fringilla, ultricies neque vel, tempor erat. Phasellus vel sem vitae ligula viverra luctus hendrerit \n",
    "ac quam. Vivamus dignissim, diam sed porttitor lacinia, elit erat lobortis tellus, nec euismod dolor turpis a leo.'''\n",
    "\n",
    "string2 = '''Mauris felis nibh, tempor ac pulvinar sed, feugiat at neque. In enim elit, venenatis nec magna eu, vestibulum \n",
    "lobortis libero. Nullam scelerisque pulvinar ex consectetur tempor. Morbi congue quis leo auctor facilisis. Mauris \n",
    "et diam ultricies, interdum nisi ac, condimentum turpis. Donec a risus sed dolor blandit ultrices. Nulla ac ultrices \n",
    "elit. Nulla dictum metus tortor, in sollicitudin enim rutrum id. Nulla magna felis, molestie vel odio et, congue ornare \n",
    "nisi. Nunc molestie, mi non blandit sodales, neque diam varius quam, nec tempus metus neque vel nibh. Morbi sit amet \n",
    "sapien nec ipsum feugiat lacinia ut eu orci. Mauris id enim tincidunt, aliquet augue quis, vehicula libero. Donec eu \n",
    "laoreet nibh.'''\n",
    "\n",
    "string3 = '''Donec urna massa, faucibus et interdum id, facilisis interdum augue. Nunc enim nulla, tristique sit amet velit \n",
    "in, lacinia mollis mi. Nullam malesuada felis sed libero porttitor dapibus ut quis dolor. Curabitur vitae neque \n",
    "at arcu condimentum suscipit. Quisque sollicitudin est a elit sollicitudin, ac consectetur nisi blandit. Vestibulum \n",
    "rhoncus viverra orci, et porta purus maximus quis. Pellentesque consectetur velit eget orci rutrum mollis. Nullam \n",
    "condimentum vehicula ante at dignissim. Curabitur gravida, lorem in vehicula gravida, enim arcu semper nulla, \n",
    "id tempor magna eros vel tortor.'''\n",
    "\n",
    "email_list = [string1, string2, string3]\n",
    "\n",
    "bag_of_words = vectorizer.fit_transform(email_list)\n",
    "\n",
    "print vectorizer.vocabulary_.get('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Stopwords from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords: 153\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "print 'Number of stopwords: {0}'.format(len(sw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming with `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "respons\n",
      "respons\n",
      "unrespons\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "print stemmer.stem('responsiveness')\n",
    "print stemmer.stem('responsivity')\n",
    "print stemmer.stem('unresponsive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warming Up with `parseOutText()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hi Everyone  If you can read this message youre properly using parseOutText  Please proceed to the next part of the project\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def parseOutText(f):\n",
    "    \"\"\" given an opened email file f, parse out all text below the\n",
    "        metadata block at the top\n",
    "        (in Part 2, you will also add stemming capabilities)\n",
    "        and return a string that contains all the words\n",
    "        in the email (space-separated) \n",
    "        \n",
    "        example use case:\n",
    "        f = open(\"email_file_name.txt\", \"r\")\n",
    "        text = parseOutText(f)\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    f.seek(0)  ### go back to beginning of file (annoying)\n",
    "    all_text = f.read()\n",
    "\n",
    "    ### split off metadata\n",
    "    content = all_text.split(\"X-FileName:\")\n",
    "    words = \"\"\n",
    "    if len(content) > 1:\n",
    "        ### remove punctuation\n",
    "        text_string = content[1].translate(string.maketrans(\"\", \"\"), string.punctuation)\n",
    "\n",
    "        words = text_string\n",
    "        \n",
    "    return words\n",
    "    \n",
    "\n",
    "ff = open(\"../text_learning/test_email.txt\", \"r\")\n",
    "text = parseOutText(ff)\n",
    "print text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi everyon if you can read this messag your proper use parseouttext pleas proceed to the next part of the project\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def parseOutText(f):\n",
    "    '''\n",
    "    Input: a file containing text\n",
    "    \n",
    "    Output: the stemmed words in the input text, all separated by a single space\n",
    "    '''\n",
    "    f.seek(0)  ### go back to beginning of file (annoying)\n",
    "    all_text = f.read()\n",
    "\n",
    "    ### split off metadata\n",
    "    content = all_text.split(\"X-FileName:\")\n",
    "    \n",
    "    # the stemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    # the string of words\n",
    "    words = \"\"\n",
    "    \n",
    "    if len(content) > 1:\n",
    "        ### remove punctuation\n",
    "        text_string = content[1].translate(string.maketrans(\"\", \"\"), string.punctuation)\n",
    "\n",
    "        ### split the text string into individual words, stem each word,\n",
    "        ### and append the stemmed word to words (make sure there's a single\n",
    "        ### space between each stemmed word)\n",
    "        for word in text_string.split():\n",
    "            # stem the word and add it to words\n",
    "            words += stemmer.stem(word) + ' '       \n",
    "        \n",
    "    return words[:-1]\n",
    "    \n",
    "\n",
    "ff = open(\"../text_learning/test_email.txt\", \"r\")\n",
    "text = parseOutText(ff)\n",
    "print text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Away \"Signature Words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tjonesnsf stephani and sam need nymex calendar\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\"\"\"\n",
    "    Starter code to process the emails from Sara and Chris to extract\n",
    "    the features and get the documents ready for classification.\n",
    "\n",
    "    The list of all the emails from Sara are in the from_sara list\n",
    "    likewise for emails from Chris (from_chris)\n",
    "\n",
    "    The actual documents are in the Enron email dataset, which\n",
    "    you downloaded/unpacked in Part 0 of the first mini-project. If you have\n",
    "    not obtained the Enron email corpus, run startup.py in the tools folder.\n",
    "\n",
    "    The data is stored in lists and packed away in pickle files at the end.\n",
    "\"\"\"\n",
    "\n",
    "sw = [\"sara\", \"shackleton\", \"chris\", \"germani\"]\n",
    "with open(\"from_sara.txt\", \"r\") as from_sara, open(\"from_chris.txt\", \"r\") as from_chris:\n",
    "\n",
    "    from_data = []\n",
    "    word_data = []\n",
    "\n",
    "    ### temp_counter is a way to speed up the development--there are\n",
    "    ### thousands of emails from Sara and Chris, so running over all of them\n",
    "    ### can take a long time\n",
    "    ### temp_counter helps you only look at the first 200 emails in the list so you\n",
    "    ### can iterate your modifications quicker\n",
    "    temp_counter = 0\n",
    "\n",
    "\n",
    "    for name, from_person in [(\"sara\", from_sara), (\"chris\", from_chris)]:\n",
    "        for path in from_person:\n",
    "            ### only look at first 200 emails when developing\n",
    "            ### once everything is working, remove this line to run over full dataset\n",
    "            \n",
    "            #temp_counter += 1\n",
    "            if temp_counter < 200:\n",
    "                path = os.path.join('..', path[:-1])\n",
    "\n",
    "                with open(path, 'r') as email:\n",
    "                    ### use parseOutText to extract the text from the opened email\n",
    "                    text = parseOutText(email)\n",
    "\n",
    "                    ### use str.replace() to remove any instances of the words\n",
    "                    ### [\"sara\", \"shackleton \", \"chris\", \"germani\"]\n",
    "                    for word in sw:\n",
    "                        if(word in text):\n",
    "                            text = text.replace(word, \"\")\n",
    "\n",
    "                    ### append the text to word_data\n",
    "                    word_data.append(text.replace('\\n',' ').strip())\n",
    "\n",
    "                    ### append a 0 to from_data if email is from Sara, and 1 if email is from Chris\n",
    "                    if name=='sara':\n",
    "                        from_data.append(0)\n",
    "                    else:\n",
    "                        from_data.append(1)\n",
    "\n",
    "pickle.dump( word_data, open(\"your_word_data.pkl\", \"w\") )\n",
    "pickle.dump( from_data, open(\"your_email_authors.pkl\", \"w\") )\n",
    "\n",
    "print word_data[152]\n",
    "\n",
    "# word data is a list of strings that we get by stemming the emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `TfIdf` it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different words: 38757\n"
     ]
    }
   ],
   "source": [
    "### in Part 4, do TfIdf vectorization here\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
    "vectorizer.fit_transform(word_data)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "print 'Number of different words: {0}'.format(len(feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing `TfIdf` features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stephaniethank\n"
     ]
    }
   ],
   "source": [
    "print feature_names[34597]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
